{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programmer : Aditya Rokade\n",
    "#College : Keystone School of Engineering\n",
    "#Year  : TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I ate the entire apple and I would eat the entire cake too. Just to be the person who eats enough to wear a size that starts with a 'd'.\"\n",
    "sentence2 = \"She wears her heart on her sleeve and dreams of castles in the sky. But for now, she'll listen to sad songs in the pouring rain, hoping for a love that won't make her cry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeninzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words -  ['I', 'ate', 'the', 'entire', 'apple', 'and', 'I', 'would', 'eat', 'the', 'entire', 'cake', 'too', '.', 'Just', 'to', 'be', 'the', 'person', 'who', 'eats', 'enough', 'to', 'wear', 'a', 'size', 'that', 'starts', 'with', 'a', \"'d\", \"'\", '.']\n",
      "Tokenized Sentences -  ['I ate the entire apple and I would eat the entire cake too.', \"Just to be the person who eats enough to wear a size that starts with a 'd'.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "print(\"Tokenized Words - \", word_tokenize(sentence1))\n",
    "print(\"Tokenized Sentences - \", sent_tokenize(sentence1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging : \n",
      "[('I', 'PRP'), ('ate', 'VBP'), ('the', 'DT'), ('entire', 'JJ'), ('apple', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('would', 'MD'), ('eat', 'VB'), ('the', 'DT'), ('entire', 'JJ'), ('cake', 'NN'), ('too', 'RB'), ('.', '.'), ('Just', 'NNP'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('person', 'NN'), ('who', 'WP'), ('eats', 'VBZ'), ('enough', 'RB'), ('to', 'TO'), ('wear', 'VB'), ('a', 'DT'), ('size', 'NN'), ('that', 'WDT'), ('starts', 'VBZ'), ('with', 'IN'), ('a', 'DT'), (\"'d\", 'NN'), (\"'\", 'POS'), ('.', '.'), ('She', 'PRP'), ('wears', 'VBZ'), ('her', 'PRP'), ('heart', 'NN'), ('on', 'IN'), ('her', 'PRP$'), ('sleeve', 'NN'), ('and', 'CC'), ('dreams', 'NNS'), ('of', 'IN'), ('castles', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('sky', 'NN'), ('.', '.'), ('But', 'CC'), ('for', 'IN'), ('now', 'RB'), (',', ','), ('she', 'PRP'), (\"'ll\", 'MD'), ('listen', 'VB'), ('to', 'TO'), ('sad', 'JJ'), ('songs', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('pouring', 'NN'), ('rain', 'NN'), (',', ','), ('hoping', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('love', 'NN'), ('that', 'WDT'), ('wo', 'MD'), (\"n't\", 'RB'), ('make', 'VB'), ('her', 'PRP$'), ('cry', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "sent = word_tokenize(sentence1) + word_tokenize(sentence2)\n",
    "print(\"POS Tagging : \")\n",
    "print(pos_tag(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Token :  ['She', 'wears', 'her', 'heart', 'on', 'her', 'sleeve', 'and', 'dreams', 'of', 'castles', 'in', 'the', 'sky', '.', 'But', 'for', 'now', ',', 'she', \"'ll\", 'listen', 'to', 'sad', 'songs', 'in', 'the', 'pouring', 'rain', ',', 'hoping', 'for', 'a', 'love', 'that', 'wo', \"n't\", 'make', 'her', 'cry', '.']\n",
      "\n",
      "Cleaned Token : ['She', 'wears', 'heart', 'sleeve', 'dreams', 'castles', 'sky', '.', 'But', ',', \"'ll\", 'listen', 'sad', 'songs', 'pouring', 'rain', ',', 'hoping', 'love', 'wo', \"n't\", 'make', 'cry', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "cleaned_token = []\n",
    "\n",
    "for i in token:\n",
    "    if i not in stop_words:\n",
    "        cleaned_token.append(i)\n",
    "\n",
    "print(\"Raw Token : \", token)\n",
    "print(\"\\nCleaned Token :\", cleaned_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'wear', 'heart', 'sleev', 'dream', 'castl', 'sky', '.', 'but', ',', \"'ll\", 'listen', 'sad', 'song', 'pour', 'rain', ',', 'hope', 'love', 'wo', \"n't\", 'make', 'cri', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = []\n",
    "\n",
    "for token in cleaned_token:\n",
    "  stemmed = stemmer.stem(token)\n",
    "  stemmed_tokens.append(stemmed)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'wear', 'heart', 'sleeve', 'dream', 'castle', 'sky', '.', 'But', ',', \"'ll\", 'listen', 'sad', 'song', 'pouring', 'rain', ',', 'hoping', 'love', 'wo', \"n't\", 'make', 'cry', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = []\n",
    "for token in cleaned_token:\n",
    "  lemmatized = lemmatizer.lemmatize(token)\n",
    "  lemmatized_tokens.append(lemmatized)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 28)\t0.15249857033260467\n",
      "  (0, 27)\t0.457495710997814\n",
      "  (0, 26)\t0.15249857033260467\n",
      "  (0, 25)\t0.15249857033260467\n",
      "  (0, 24)\t0.15249857033260467\n",
      "  (0, 23)\t0.15249857033260467\n",
      "  (0, 22)\t0.15249857033260467\n",
      "  (0, 21)\t0.15249857033260467\n",
      "  (0, 20)\t0.15249857033260467\n",
      "  (0, 19)\t0.15249857033260467\n",
      "  (0, 18)\t0.15249857033260467\n",
      "  (0, 17)\t0.15249857033260467\n",
      "  (0, 16)\t0.15249857033260467\n",
      "  (0, 15)\t0.15249857033260467\n",
      "  (0, 14)\t0.15249857033260467\n",
      "  (0, 13)\t0.15249857033260467\n",
      "  (0, 12)\t0.15249857033260467\n",
      "  (0, 11)\t0.15249857033260467\n",
      "  (0, 10)\t0.15249857033260467\n",
      "  (0, 9)\t0.15249857033260467\n",
      "  (0, 8)\t0.15249857033260467\n",
      "  (0, 7)\t0.30499714066520933\n",
      "  (0, 6)\t0.15249857033260467\n",
      "  (0, 5)\t0.30499714066520933\n",
      "  (0, 4)\t0.15249857033260467\n",
      "  (0, 3)\t0.15249857033260467\n",
      "  (0, 2)\t0.15249857033260467\n",
      "  (0, 1)\t0.15249857033260467\n",
      "  (0, 0)\t0.15249857033260467\n",
      "['and' 'are' 'bills' 'business' 'but' 'day' 'enough' 'have' 'hobby' 'hope'\n",
      " 'hustlea' 'idea' 'into' 'job' 'many' 'monetize' 'most' 'one' 'or'\n",
      " 'passion' 'pay' 'pour' 'sensible' 'side' 'the' 'their' 'they' 'to'\n",
      " 'which']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Most are sensible enough to have a day-job to pay the bills, but many have a side hustle\" \n",
    "    \"a hobby or a business idea into which they pour their passion and hope to one day monetize.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "matrix = vectorizer.fit(corpus)\n",
    "matrix.vocabulary_\n",
    "\n",
    "tfidf_matrix = vectorizer.transform(corpus)\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
